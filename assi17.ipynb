{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0df7c26",
   "metadata": {},
   "source": [
    "# Naïve Bayes from Scratch\n",
    "To implement Naïve Bayes manually we compute:\n",
    "- **Class priors**: probability that a message is spam or normal.\n",
    "- **Likelihoods**: probability of each word appearing in each class. We apply Laplace smoothing so unseen words do not zero out the probability.\n",
    "When classifying a new message we split it into tokens, sum the log-probabilities for each class, and choose the larger value. This simple approach works well for text because many token probabilities are small yet independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f05089",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"limited time offer claim your prize now\",\n",
    "        \"meeting reminder for project update\",\n",
    "        \"win cash by entering free lottery\",\n",
    "        \"family dinner plans for saturday\",\n",
    "        \"exclusive deal just for you click\",\n",
    "        \"invoice attached for last month\",\n",
    "        \"cheap meds available order today\",\n",
    "        \"team outing scheduled at 5pm\",\n",
    "        \"congratulations you have won a voucher\",\n",
    "        \"please review the attached report\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "train_df = records.sample(frac=0.7, random_state=42)\n",
    "test_df = records.drop(train_df.index)\n",
    "\n",
    "print(\"Training samples:\", len(train_df))\n",
    "print(\"Testing samples:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3180d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "classes = sorted(train_df[\"label\"].unique())\n",
    "class_priors = {}\n",
    "word_counts = {cls: Counter() for cls in classes}\n",
    "total_words = {cls: 0 for cls in classes}\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    label = row[\"label\"]\n",
    "    tokens = tokenize(row[\"text\"])\n",
    "    class_priors[label] = class_priors.get(label, 0) + 1\n",
    "    word_counts[label].update(tokens)\n",
    "    total_words[label] += len(tokens)\n",
    "\n",
    "total_docs = len(train_df)\n",
    "vocab = set(token for counts in word_counts.values() for token in counts)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "for cls in classes:\n",
    "    class_priors[cls] = math.log(class_priors[cls] / total_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokens = tokenize(text)\n",
    "    scores = {}\n",
    "    for cls in classes:\n",
    "        score = class_priors[cls]\n",
    "        for token in tokens:\n",
    "            count = word_counts[cls][token]\n",
    "            score += math.log((count + 1) / (total_words[cls] + vocab_size))\n",
    "        scores[cls] = score\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "predictions = test_df[\"text\"].apply(predict)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(test_df[\"label\"], predictions))\n",
    "\n",
    "print(\"\\nDetailed report:\")\n",
    "print(classification_report(test_df[\"label\"], predictions, target_names=[\"normal\", \"spam\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
