{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125485b7",
   "metadata": {},
   "source": [
    "# Polynomial Kernel SVM from Scratch for Student Performance\n",
    "Kernelised SVMs allow non-linear decision boundaries by projecting data into a higher-dimensional feature space. Instead of relying on a library implementation, we manually expand the original features with all degree-2 polynomial terms (squares and pairwise products). Fitting a linear SVM on this expanded space is equivalent to using a polynomial kernel. We again optimise hinge-loss with stochastic sub-gradient descent and then transform predictions back to pass/fail labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"study_time\": [1, 2, 3, 4, 5, 2, 6, 7, 3, 8, 5, 4],\n",
    "    \"absences\": [8, 6, 5, 4, 3, 9, 2, 1, 7, 1, 3, 2],\n",
    "    \"internal_score\": [50, 55, 60, 65, 70, 48, 75, 80, 58, 85, 72, 68],\n",
    "    \"passed\": [0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "train_df = data.sample(frac=0.75, random_state=7)\n",
    "test_df = data.drop(train_df.index)\n",
    "\n",
    "print(\"Training size:\", len(train_df))\n",
    "print(\"Testing size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b317c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X):\n",
    "    \"\"\"Expand features to degree-2 polynomial terms (simulates a polynomial kernel).\"\"\"\n",
    "    linear = X\n",
    "    squares = X ** 2\n",
    "    interactions = []\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(i + 1, X.shape[1]):\n",
    "            interactions.append((X[:, i] * X[:, j]).reshape(-1, 1))\n",
    "    if interactions:\n",
    "        interactions = np.hstack(interactions)\n",
    "        return np.hstack([linear, squares, interactions])\n",
    "    return np.hstack([linear, squares])\n",
    "\n",
    "X_train_raw = train_df[[\"study_time\", \"absences\", \"internal_score\"]].to_numpy().astype(float)\n",
    "X_test_raw = test_df[[\"study_time\", \"absences\", \"internal_score\"]].to_numpy().astype(float)\n",
    "\n",
    "# Scale features to roughly similar ranges\n",
    "X_train_raw = (X_train_raw - X_train_raw.mean(axis=0)) / X_train_raw.std(axis=0)\n",
    "X_test_raw = (X_test_raw - X_train_raw.mean(axis=0)) / X_train_raw.std(axis=0)\n",
    "\n",
    "X_train = polynomial_features(X_train_raw)\n",
    "X_test = polynomial_features(X_test_raw)\n",
    "\n",
    "y_train = train_df[\"passed\"].apply(lambda v: 1 if v == 1 else -1).to_numpy()\n",
    "y_test = test_df[\"passed\"].apply(lambda v: 1 if v == 1 else -1).to_numpy()\n",
    "\n",
    "print(\"Expanded feature dimension:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_svm(X, y, learning_rate=0.02, lambda_reg=0.02, epochs=60):\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    bias = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for xi, yi in zip(X, y):\n",
    "            condition = yi * (np.dot(xi, weights) + bias)\n",
    "            if condition >= 1:\n",
    "                # Within the margin we only decay weights due to regularisation\n",
    "                weights -= learning_rate * (2 * lambda_reg * weights)\n",
    "            else:\n",
    "                # Misclassified samples push the hyperplane toward the correct side\n",
    "                weights -= learning_rate * (2 * lambda_reg * weights - yi * xi)\n",
    "                bias += learning_rate * yi\n",
    "    return weights, bias\n",
    "\n",
    "def predict_linear_svm(X, weights, bias):\n",
    "    scores = X @ weights + bias\n",
    "    return np.where(scores >= 0, 1, -1)\n",
    "\n",
    "weights, bias = train_linear_svm(X_train, y_train)\n",
    "y_pred = predict_linear_svm(X_test, weights, bias)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix((y_test == 1).astype(int), (y_pred == 1).astype(int)))\n",
    "\n",
    "print(\"\\nDetailed report:\")\n",
    "print(classification_report((y_test == 1).astype(int), (y_pred == 1).astype(int), target_names=[\"fail\", \"pass\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
